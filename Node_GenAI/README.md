# ğŸ§  Gen AI Chatbot with RAG, LangChain & LangSmith (Node.js)

A **context-aware AI chatbot** built with **Node.js**, **LangChain**, **LangSmith**, and **Retrieval-Augmented Generation (RAG)**.  
This project integrates **memory** for persistent conversation history and a **RAG pipeline** to retrieve and ground responses in relevant external knowledge.

---

## ğŸš€ Features

- ğŸ’¬ **Conversational Memory** â€” The chatbot stores and recalls previous messages for contextual continuity.  
- ğŸ“š **Retrieval-Augmented Generation (RAG)** â€” Fetches relevant documents or data chunks before generating answers.  
- âš™ï¸ **LangChain Integration** â€” Modular chain and agent orchestration for natural language processing.  
- ğŸ§© **LangSmith Integration** â€” Tracing, debugging, and monitoring of LLM pipelines.  
- ğŸ§  **Custom Knowledge Base** â€” Upload your own data (PDFs, text files, or structured data) for domain-specific chat.  
- ğŸŒ **REST API Support** â€” Easy to integrate with frontends or other applications.  
- ğŸ” **Secure Environment Variables** â€” API keys and configuration managed via `.env`.

---

## ğŸ—ï¸ Architecture Overview

```plaintext
User
 â”‚
 â–¼
Chat Interface / API
 â”‚
 â–¼
LangChain Pipeline
 â”‚ â”œâ”€â”€ Conversational Memory (BufferMemory / Redis)
 â”‚ â”œâ”€â”€ Retriever (e.g., Chroma, Pinecone, or FAISS)
 â”‚ â””â”€â”€ LLM (OpenAI / Anthropic / Gemini / Llama)
 â”‚
 â–¼
LangSmith (Tracing & Evaluation)
 â”‚
 â–¼
Response to User
```

---

## ğŸ§© Tech Stack

| Component | Description |
|------------|--------------|
| **Node.js** | Backend runtime environment |
| **LangChain.js** | Orchestrates the LLM, retriever, and memory |
| **LangSmith** | Debugs, monitors, and traces LLM chains |
| **Vector Store (e.g., Chroma / Pinecone)** | Stores embeddings for RAG |
| **OpenAI / Anthropic API** | Large Language Model provider |
| **Express.js** | RESTful API framework |

---

## âš™ï¸ Installation

### 1. Clone the Repository
```bash
git clone https://github.com/<your-username>/genai-chatbot.git
cd genai-chatbot
```

### 2. Install Dependencies
```bash
npm install
```

### 3. Configure Environment Variables
Create a `.env` file in the project root:

```bash
OPENAI_API_KEY=your_openai_api_key
LANGCHAIN_API_KEY=your_langchain_api_key
LANGSMITH_API_KEY=your_langsmith_api_key
VECTOR_DB_URL=your_vector_db_endpoint
PORT=3000
```

### 4. Run the App
```bash
npm run dev
```

The chatbot will be available at **http://localhost:3000**

---

## ğŸ’¬ Example API Usage

**Endpoint:** `POST /chat`

**Request:**
```json
{
  "userId": "user_001",
  "message": "What did we discuss about project deadlines?"
}
```

**Response:**
```json
{
  "reply": "We discussed that the project deadline is October 31st and the review phase starts next week."
}
```

---

## ğŸ§  How It Works

1. **User sends a query** â†’ The system retrieves recent chat history for context.  
2. **Retriever fetches relevant documents** â†’ Uses vector embeddings (via FAISS/Chroma/Pinecone).  
3. **LLM generates an answer** â†’ The model combines retrieved info + memory context.  
4. **Response stored in conversation history** â†’ Future queries build upon it.  
5. **LangSmith logs & traces** â†’ Enables debugging, evaluation, and chain optimization.

---

## ğŸ§ª Example Workflow

```bash
# Step 1: Embed your knowledge base
npm run embed:data

# Step 2: Start chatbot API
npm start

# Step 3: Send queries via API or frontend
curl -X POST http://localhost:3000/chat -H "Content-Type: application/json" -d '{"userId":"u123","message":"Summarize the latest sales report."}'
```

---

## ğŸ§° Folder Structure

```plaintext
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ index.js           # App entry point
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â””â”€â”€ chat.js        # Chat API route
â”‚   â”œâ”€â”€ chains/
â”‚   â”‚   â””â”€â”€ ragChain.js    # LangChain RAG setup
â”‚   â”œâ”€â”€ memory/
â”‚   â”‚   â””â”€â”€ memoryManager.js
â”‚   â”œâ”€â”€ retriever/
â”‚   â”‚   â””â”€â”€ vectorStore.js
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logger.js
â”œâ”€â”€ data/                  # Your documents or dataset
â”œâ”€â”€ .env
â”œâ”€â”€ package.json
â””â”€â”€ README.md
```

---

## ğŸ“ˆ Future Enhancements

- ğŸ§© Integrate WebSocket for real-time chat  
- ğŸ’¾ Use Redis / Postgres for persistent memory  
- ğŸ” Add semantic search to improve RAG accuracy  
- ğŸŒ Deploy on Vercel / Render / AWS Lambda  

---

## ğŸ¤ Contributing

Contributions are welcome!  
Fork the repo, create a new branch, and submit a pull request.

```bash
git checkout -b feature/new-feature
git commit -m "Add new feature"
git push origin feature/new-feature
```

---

## ğŸŒŸ Acknowledgements

- [LangChain.js](https://js.langchain.com/)
- [LangSmith](https://smith.langchain.com/)
- [OpenAI API](https://platform.openai.com/)
- [Chroma / Pinecone](https://www.pinecone.io/)
- [Node.js](https://nodejs.org/)

---

_This README was generated on October 17, 2025_
